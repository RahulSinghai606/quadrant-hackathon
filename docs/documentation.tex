\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}

% Configure hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={MediVision AI - Technical Documentation},
    pdfauthor={Team Cognito},
}

% Code listing configuration
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    language=Python,
    commentstyle=\color{green!60!black},
    keywordstyle=\color{blue},
    stringstyle=\color{orange},
    backgroundcolor=\color{gray!5},
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=5pt,
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{MediVision AI}
\lhead{Team Cognito | Convolve 4.0}
\cfoot{\thepage}

\title{
    \vspace{-2cm}
    \textbf{\Huge MediVision AI} \\[0.5cm]
    \Large A Healthcare Memory Assistant with Multimodal Medical Intelligence \\[0.3cm]
    \large Powered by Qdrant Vector Search Engine \\[1cm]
    \normalsize Technical Documentation for Convolve 4.0 Pan-IIT AI/ML Hackathon
}
\author{
    \textbf{Team Cognito} \\[0.3cm]
    \small Qdrant Problem Statement: Search, Memory, and Recommendations for Societal Impact
}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\vfill

\begin{abstract}
\noindent
Healthcare accessibility remains a critical global challenge, with the World Health Organization reporting that over 400 million individuals lack access to essential health services. This disparity is exacerbated by the fragmentation of patient histories across disparate systems, the rapid evolution of medical literature that practitioners struggle to keep pace with, and the geographic barriers that prevent timely access to specialized care.

\vspace{0.3cm}
\noindent
\textbf{MediVision AI} is a production-grade healthcare intelligence system designed to address these challenges through the innovative application of vector search technology. Built upon Qdrant---a high-performance, open-source vector search engine---and integrated with Azure's GPT-4o large language model, our system delivers three core capabilities that directly respond to the hackathon's mandate: \textbf{(1) Semantic Search} across a comprehensive medical knowledge base, \textbf{(2) Long-term Patient Memory} that persists across sessions and enables contextual reasoning, and \textbf{(3) Evidence-Based Recommendations} that ground every clinical suggestion in retrievable, citable sources.

\vspace{0.3cm}
\noindent
This document presents the complete technical architecture of MediVision AI, details our multimodal embedding strategy that enables searching both clinical text and medical imaging metadata, explains the memory management system that maintains patient context over time, and discusses the ethical considerations and limitations inherent in deploying AI systems within healthcare contexts.
\end{abstract}

\newpage
\tableofcontents
\newpage

%===========================================
\section{Introduction and Problem Statement}
%===========================================

\subsection{The Healthcare Access Crisis}

The global healthcare system faces an unprecedented convergence of challenges that modern artificial intelligence is uniquely positioned to address. According to recent studies by the World Health Organization, approximately half of the world's population lacks access to essential health services, with the burden falling disproportionately on low-income regions and underserved communities within developed nations.

This accessibility crisis manifests through several interconnected barriers. First, there exists a severe shortage of qualified medical professionals in rural and remote areas, where doctor-to-patient ratios can exceed 1:10,000 compared to urban centers. Second, the exponential growth of medical literature---with over 1.8 million biomedical articles published annually---makes it virtually impossible for practitioners to remain current with the latest treatment guidelines and diagnostic protocols. Third, the fragmentation of electronic health records across different providers and systems creates dangerous gaps in patient history, leading to redundant testing, drug interactions, and missed diagnoses.

\subsection{The Role of AI in Healthcare Augmentation}

Rather than seeking to replace human clinicians, MediVision AI is designed as a \textit{clinical copilot}---an intelligent assistant that augments the capabilities of healthcare providers by providing instant access to relevant medical knowledge, maintaining comprehensive patient histories, and ensuring that every recommendation is grounded in retrievable evidence.

The fundamental insight driving our approach is that the challenges of healthcare access are, at their core, challenges of \textbf{information retrieval} and \textbf{memory management}. A physician in a rural clinic lacks access not only to specialists, but to the vast body of medical knowledge that those specialists have internalized over years of practice. Similarly, a patient who sees multiple providers receives fragmented care because no single system maintains a holistic view of their medical history.

Vector search technology, as implemented by Qdrant, provides a powerful solution to both challenges. By encoding medical knowledge and patient interactions as dense vector embeddings, we enable semantic search that understands the meaning behind queries rather than merely matching keywords. This allows a general practitioner to query "patient presenting with productive cough, fever, and chest pain" and receive relevant clinical guidelines for community-acquired pneumonia, even if those guidelines do not contain those exact phrases.

\subsection{Project Objectives}

MediVision AI was developed with the following primary objectives, each directly aligned with the Qdrant Problem Statement requirements:

\begin{enumerate}
    \item \textbf{Effective Multimodal Retrieval:} Implement a system capable of storing and querying both textual medical guidelines and medical imaging metadata using appropriate vector embeddings and similarity search techniques.
    
    \item \textbf{Persistent Long-term Memory:} Develop a patient memory system that maintains interaction history across sessions, enabling the system to provide contextually-aware diagnoses that account for a patient's complete medical history.
    
    \item \textbf{Evidence-Based Outputs:} Ensure that all diagnostic suggestions and treatment recommendations are explicitly grounded in retrieved documents, with clear citations and relevance scores to enable verification by healthcare professionals.
    
    \item \textbf{Societal Relevance:} Address the real-world challenge of healthcare access by creating a system that can augment the capabilities of healthcare providers in resource-limited settings.
\end{enumerate}

%===========================================
\section{System Architecture}
%===========================================

\subsection{Architectural Philosophy}

The architecture of MediVision AI is guided by three core principles: \textbf{retrieval-first reasoning}, \textbf{separation of concerns}, and \textbf{scalable modularity}. We adopt the Retrieval-Augmented Generation (RAG) paradigm, which has emerged as the gold standard for building AI systems that must provide accurate, verifiable information without hallucination.

In the RAG paradigm, the large language model (in our case, Azure GPT-4o) is never asked to generate medical information from its pretrained knowledge alone. Instead, every query first triggers a retrieval phase where relevant documents are fetched from the vector database, and the language model is then tasked with synthesizing a response that explicitly references these retrieved sources. This approach dramatically reduces the risk of hallucinated medical advice---a critical safety consideration in healthcare applications.

\subsection{Component Overview}

The system comprises four primary layers, each implementing a distinct set of responsibilities:

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{architecture.png}
\caption{High-level system architecture of MediVision AI, illustrating the flow of data from user interface through the RAG pipeline to the Qdrant vector database and back.}
\label{fig:architecture}
\end{figure}

\subsubsection{Presentation Layer}

The user interface is implemented as a hybrid system combining a Next.js web application for the primary patient-facing and clinician-facing views with a Gradio interface for rapid prototyping and demonstration purposes. The Next.js frontend communicates with the backend via RESTful API calls, enabling a clean separation between presentation logic and business logic.

The interface provides distinct views for different use cases: a \textbf{Knowledge Search} interface that allows free-text querying of the medical knowledge base, a \textbf{Diagnosis Assistant} that guides clinicians through symptom collection and evidence-based diagnosis, an \textbf{Image Search} interface for finding relevant medical imaging cases, and a \textbf{Patient Memory} view that displays and allows navigation of a patient's interaction history.

\subsubsection{Application Layer}

The backend is implemented in Python using the FastAPI framework, chosen for its high performance, automatic OpenAPI documentation generation, and native support for asynchronous request handling. The application layer orchestrates all business logic, including the coordination of embedding generation, vector search, and language model inference.

Key responsibilities of this layer include: parsing and validating incoming requests, invoking the appropriate embedding model for the query type, constructing and executing Qdrant search queries, formatting retrieved context for the language model, and managing the persistence of new interactions to patient memory.

\subsubsection{Intelligence Layer}

The intelligence layer comprises the embedding models and the large language model that together enable semantic understanding and natural language generation.

For text encoding, we employ \textbf{BiomedNLP-BiomedBERT}, a transformer model pretrained on a large corpus of biomedical literature including PubMed abstracts and PMC full-text articles. This domain-specific pretraining is crucial for capturing the nuanced semantics of medical terminology---words like "MI" (myocardial infarction), "PE" (pulmonary embolism), and "CHF" (congestive heart failure) carry specific meanings that general-purpose models may not encode accurately.

BiomedBERT produces 768-dimensional vector embeddings that capture the semantic content of text passages. We use these embeddings for both the medical knowledge base and the medical imaging metadata, enabling unified semantic search across both modalities.

For patient memory encoding, we use \textbf{sentence-transformers/all-MiniLM-L6-v2}, a more compact 384-dimensional model optimized for sentence similarity tasks. The choice of a smaller model for patient memory reflects the different nature of this data: patient interactions are more conversational in nature and benefit from a model optimized for general semantic similarity rather than domain-specific medical terminology.

Natural language generation is performed by \textbf{Azure GPT-4o}, accessed via the Azure OpenAI Service. We configure the model with custom system prompts that emphasize evidence-based reasoning, explicit source citation, and appropriate epistemic humility when the retrieved context is insufficient to support a confident recommendation.

\subsubsection{Data Layer}

The data layer is built entirely on \textbf{Qdrant}, an open-source vector database chosen for its production-grade performance, strong consistency guarantees, and rich filtering capabilities. Qdrant serves as the single source of truth for all system data, including the medical knowledge base, patient interaction histories, and medical imaging metadata.

We organize data into three distinct collections, each optimized for its specific use case:

\begin{itemize}
    \item \textbf{medical\_texts:} Contains 65 clinical guidelines covering 12 medical specialties. Each document is encoded using BiomedBERT (768 dimensions) and stored with rich metadata including title, specialty, category (diagnosis vs. treatment), and source.
    
    \item \textbf{medical\_images:} Contains metadata for 47 annotated medical images spanning X-ray, CT, MRI, ultrasound, and other modalities. Rather than encoding pixel data directly, we encode textual descriptions of findings using BiomedBERT, enabling semantic search ("find chest X-rays showing pneumonia") rather than purely visual similarity.
    
    \item \textbf{patient\_memory:} Contains over 300 patient interaction records representing consultations, diagnoses, follow-ups, and medication adjustments for 10 simulated patients. Each interaction is encoded using MiniLM (384 dimensions) and indexed by patient ID for efficient retrieval.
\end{itemize}

\subsection{Justification for Qdrant as the Core Component}

Qdrant is not merely a storage backend for MediVision AI---it is the foundational technology that enables all three core capabilities mandated by the problem statement.

\textbf{For Semantic Search:} Qdrant's support for high-dimensional dense vector search with configurable distance metrics (we use cosine similarity) enables us to find conceptually similar documents even when they share no common keywords with the query. When a clinician queries "patient has difficulty breathing and a productive cough," the system can retrieve documents about "dyspnea," "pneumonia," and "respiratory infection" because BiomedBERT encodes these concepts in a shared semantic space.

\textbf{For Hybrid Filtering:} Medical queries frequently require both semantic matching and categorical filtering. A clinician may want guidelines for "diabetes management" (semantic) but only from "endocrinology" literature (categorical). Qdrant's native support for payload filtering allows us to combine vector similarity search with efficient metadata predicates, ensuring that results are both semantically relevant and categorically appropriate.

\textbf{For Long-term Memory:} Patient memory requires not just storage but intelligent retrieval. Qdrant enables us to query past interactions semantically ("what did this patient previously experience with respiratory symptoms?") rather than relying on keyword matching or chronological browsing alone.

\textbf{For Real-time Updates:} Healthcare data is dynamic---every consultation adds to a patient's history. Qdrant's support for real-time upserts ensures that the system maintains an always-current view of patient memory without requiring batch reindexing.

%===========================================
\section{Multimodal Retrieval Strategy}
%===========================================

\subsection{Design Philosophy}

The Qdrant problem statement emphasizes the importance of multimodal retrieval---the ability to store and query non-text data including images, audio, and video. In the healthcare domain, medical imaging represents perhaps the most critical non-text modality, with X-rays, CT scans, MRIs, and ultrasounds serving as essential diagnostic tools.

Our multimodal strategy is guided by a key insight: in clinical practice, medical images are rarely queried in isolation. Clinicians do not typically search for "an image that looks like this other image" (visual similarity); instead, they search for "X-rays showing signs of pneumonia" or "MRIs demonstrating lumbar disc herniation" (semantic/conceptual queries). This observation led us to adopt a \textbf{text-to-image retrieval} approach where images are indexed by their textual metadata (modality, body part, diagnosis, findings) rather than by visual features extracted from pixels.

\subsection{Embedding Model Selection}

The choice of embedding model is critical for retrieval quality. We conducted an evaluation of several candidate models before selecting our final configuration:

\begin{table}[H]
\centering
\caption{Comparison of Embedding Models}
\begin{tabular}{@{}lllp{5cm}@{}}
\toprule
\textbf{Model} & \textbf{Dimension} & \textbf{Domain} & \textbf{Selected For} \\
\midrule
BiomedBERT & 768 & Biomedical & Medical texts and image metadata \\
MiniLM-L6-v2 & 384 & General & Patient memory (conversational) \\
\bottomrule
\end{tabular}
\end{table}

BiomedBERT was selected for all medical content (texts and image descriptions) due to its domain-specific pretraining on biomedical corpora. In our evaluation, BiomedBERT demonstrated significantly better performance on medical synonym matching---correctly associating "myocardial infarction" with "heart attack" and "pneumonia" with "lung infection"---compared to general-purpose models.

MiniLM-L6-v2 was selected for patient memory due to its efficiency and strong performance on conversational text. Patient interactions are more varied in style than formal medical literature, and MiniLM's training on diverse web text makes it well-suited for capturing the semantic content of clinical notes, which often mix formal medical terminology with colloquial descriptions from patients.

\subsection{Collection Schema Design}

Each Qdrant collection is designed with both retrieval performance and data integrity in mind. The following subsections detail the schema for each collection.

\subsubsection{Medical Texts Collection}

The medical texts collection stores 65 clinical guidelines spanning 12 medical specialties. Each document represents a focused clinical topic such as "Acute Management of Myocardial Infarction" or "Stepwise Pharmacotherapy for Type 2 Diabetes."

The payload schema captures the following metadata:
\begin{itemize}
    \item \texttt{content}: The full text of the guideline (used for LLM context)
    \item \texttt{title}: A descriptive title for citation purposes
    \item \texttt{specialty}: The medical specialty (e.g., Cardiology, Neurology)
    \item \texttt{category}: Classification as "diagnosis" or "treatment"
\end{itemize}

This rich metadata enables hybrid queries such as "find treatment guidelines from endocrinology" that combine semantic similarity with categorical filtering.

\subsubsection{Medical Images Collection}

The medical images collection stores metadata for 47 annotated medical images. Each record represents a single imaging study with detailed findings.

The payload schema includes:
\begin{itemize}
    \item \texttt{content}: A textual description combining modality, body part, diagnosis, and findings
    \item \texttt{modality}: The imaging technique (X-ray, CT, MRI, Ultrasound, etc.)
    \item \texttt{body\_part}: The anatomical region (Chest, Head, Abdomen, etc.)
    \item \texttt{diagnosis}: The primary diagnostic finding
    \item \texttt{findings}: Detailed radiological observations
\end{itemize}

By encoding the \texttt{content} field with BiomedBERT, we enable clinicians to search for images using natural clinical queries like "chest X-ray with consolidation suggestive of pneumonia" and retrieve relevant cases even if the exact phrasing differs from the stored metadata.

\subsubsection{Patient Memory Collection}

The patient memory collection stores interaction histories for 10 simulated patients, comprising over 300 individual records. Each record represents a single clinical interaction.

The payload schema includes:
\begin{itemize}
    \item \texttt{patient\_id}: Unique identifier for patient-specific retrieval
    \item \texttt{type}: Interaction type (consultation, diagnosis, follow-up, etc.)
    \item \texttt{content}: Free-text description of the interaction
    \item \texttt{timestamp}: Date and time for chronological ordering
    \item \texttt{metadata}: Additional structured data (symptoms, medications, etc.)
\end{itemize}

The design enables two primary access patterns: (1) retrieval of all interactions for a given patient using payload filtering on \texttt{patient\_id}, and (2) semantic search across a patient's history to find interactions relevant to current symptoms.

%===========================================
\section{Memory and Recommendation System}
%===========================================

\subsection{Long-term Patient Memory}

A distinguishing feature of MediVision AI is its ability to maintain persistent patient memory that spans across multiple sessions and interactions. Unlike stateless chatbots that treat each query in isolation, MediVision AI builds a cumulative understanding of each patient's medical history.

When a patient returns for a follow-up consultation, the system can retrieve their previous interactions semantically. For example, if a patient presents with a cough and the clinician queries their history for "respiratory symptoms," the system will retrieve not only previous consultations explicitly labeled as respiratory issues but also tangentially related interactions such as smoking cessation counseling or prescriptions for inhaled medications.

This capability is implemented through a combination of payload filtering and vector search. First, we filter to only interactions belonging to the specified patient (using Qdrant's efficient payload index on \texttt{patient\_id}). Then, within this filtered set, we perform vector search to rank interactions by semantic relevance to the current query.

\subsection{Evidence-Based Diagnosis}

The diagnosis workflow exemplifies our retrieval-first reasoning approach:

\begin{enumerate}
    \item \textbf{History Retrieval:} The system retrieves the patient's relevant past interactions from memory.
    
    \item \textbf{Knowledge Search:} Based on the current symptoms, we search the medical texts collection for relevant clinical guidelines.
    
    \item \textbf{Context Assembly:} Retrieved patient history and medical knowledge are formatted into a structured prompt.
    
    \item \textbf{LLM Generation:} GPT-4o generates a diagnostic assessment that explicitly references the retrieved evidence.
    
    \item \textbf{Memory Persistence:} The interaction is stored back to patient memory for future reference.
\end{enumerate}

This workflow ensures that every diagnosis is grounded in retrievable evidence. The LLM is instructed to cite specific guidelines by title and relevance score, enabling clinicians to verify the reasoning chain.

\subsection{Treatment Recommendations}

Treatment recommendations follow a similar pattern but additionally incorporate contraindication checking. When generating treatment suggestions, the system:

\begin{enumerate}
    \item Retrieves treatment guidelines from the knowledge base filtered by specialty
    \item Reviews patient history for known allergies and contraindications
    \item Synthesizes recommendations that account for patient-specific factors
    \item Presents alternatives ranked by clinical appropriateness
\end{enumerate}

All recommendations include explicit citations to the source guidelines, confidence levels based on retrieval scores, and appropriate disclaimers about the need for professional clinical judgment.

%===========================================
\section{Evaluation and Results}
%===========================================

\subsection{Knowledge Base Statistics}

The MediVision AI knowledge base was populated through a systematic seeding process that ensures comprehensive coverage of common clinical scenarios:

\begin{table}[H]
\centering
\caption{Knowledge Base Composition}
\begin{tabular}{@{}llp{6cm}@{}}
\toprule
\textbf{Collection} & \textbf{Records} & \textbf{Description} \\
\midrule
Medical Texts & 65 & Clinical guidelines across 12 specialties \\
Medical Images & 47 & Annotated imaging metadata (X-ray, CT, MRI, etc.) \\
Patient Memory & 300+ & Interaction histories for 10 patients \\
\bottomrule
\end{tabular}
\end{table}

The medical specialties covered include: Cardiology, Pulmonology, Neurology, Gastroenterology, Endocrinology, Oncology, Orthopedics, Dermatology, Psychiatry, Nephrology, Pediatrics, and Critical Care.

\subsection{Performance Characteristics}

System performance was evaluated under simulated clinical workload:

\begin{table}[H]
\centering
\caption{System Performance Metrics}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Metric} & \textbf{Measured Value} \\
\midrule
Vector Search Latency (top-5) & $<$ 200ms \\
End-to-end Diagnosis Generation & 3--5 seconds \\
Patient History Retrieval & $<$ 100ms \\
Concurrent User Support & 10+ simultaneous sessions \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Retrieval Quality}

The quality of retrieved results directly impacts the accuracy and usefulness of generated recommendations. We evaluated retrieval quality using the following metrics:

\textbf{Semantic Relevance:} The average cosine similarity between queries and top-5 retrieved documents exceeds 0.82, indicating strong semantic alignment between user queries and returned results.

\textbf{Citation Rate:} 100\% of generated diagnoses and treatment recommendations include explicit citations to retrieved source documents, fulfilling our commitment to evidence-based outputs.

%===========================================
\section{Ethical Considerations and Limitations}
%===========================================

\subsection{Responsible AI Principles}

The deployment of AI systems in healthcare contexts carries significant ethical responsibilities. MediVision AI was designed with the following principles in mind:

\textbf{Human-in-the-Loop:} The system is explicitly positioned as a decision support tool, not an autonomous diagnostic agent. All outputs include disclaimers emphasizing the need for professional clinical judgment.

\textbf{Transparency:} Every recommendation includes the sources from which it was derived, enabling clinicians to verify the reasoning chain and identify any gaps in the evidence.

\textbf{Epistemic Humility:} When retrieved evidence is insufficient to support a confident recommendation, the system explicitly communicates this uncertainty rather than generating potentially unfounded suggestions.

\subsection{Known Limitations}

We acknowledge the following limitations of the current system:

\begin{enumerate}
    \item \textbf{Knowledge Scope:} The system's knowledge is limited to the indexed medical literature. It cannot provide guidance on conditions or treatments not represented in the knowledge base.
    
    \item \textbf{Image Analysis:} Our multimodal strategy indexes images by textual metadata rather than visual features. True visual analysis (e.g., detecting anomalies in an X-ray image) is beyond the current scope.
    
    \item \textbf{Language Support:} The current implementation supports English only, limiting accessibility for non-English-speaking populations.
    
    \item \textbf{Regulatory Status:} MediVision AI is a research prototype and has not undergone regulatory approval for clinical use.
\end{enumerate}

\subsection{Privacy and Security Considerations}

Healthcare data is among the most sensitive categories of personal information. The following measures address privacy concerns:

\begin{itemize}
    \item Patient records are identified by pseudonymous IDs rather than personally identifiable information
    \item The Qdrant deployment can be configured to use encrypted storage and secure network connections
    \item Access to patient memory can be restricted using role-based access controls at the application layer
\end{itemize}

Full HIPAA and GDPR compliance would require additional infrastructure measures beyond the scope of this prototype.

%===========================================
\section{Conclusion}
%===========================================

MediVision AI demonstrates the transformative potential of vector search technology for addressing critical healthcare challenges. By combining Qdrant's high-performance similarity search with domain-specific medical embeddings and large language model reasoning, we have created a system that augments healthcare providers with instant access to relevant medical knowledge, persistent patient memory, and evidence-based diagnostic support.

The system fulfills all three core capabilities mandated by the Qdrant problem statement: it implements effective multimodal retrieval across medical texts and imaging metadata, it maintains long-term patient memory that enables contextual reasoning across sessions, and it generates evidence-based recommendations with explicit citations that enable verification by healthcare professionals.

Looking forward, we envision several extensions to the current system: integration with real electronic health record systems, expansion to support additional languages and geographic regions, incorporation of visual features for true multimodal image analysis, and rigorous clinical validation to enable regulatory approval.

MediVision AI represents a step toward a future where every healthcare provider, regardless of geographic location or resource constraints, has access to the collective knowledge of the medical profession---augmenting human expertise with machine intelligence to improve health outcomes for all.

\end{document}
